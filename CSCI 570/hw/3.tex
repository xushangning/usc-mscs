\documentclass{article}

\usepackage[noend]{algpseudocode}
\usepackage{amsthm}
\usepackage{amsfonts}

\title{Answers to CSCI 570 - Fall 2021 - HW 3}
\author{Shangning Xu}

\begin{document}

\maketitle

\begin{enumerate}

\item The algorithm is simple: We will keep finding, connecting the shortest two ropes, putting back the connected rope and resorting until there is only one rope left.

We can implement the search for shortest ropes with a min-heap. Since every time we connect two ropes, the total number of ropes will be reduced by one, the loop has $N - 1$ iterations in total, giving a runtime performance of $\sum_{i = 2}^N O(\log i) = O(N\log N)$.

\item This is a fractional knapsack problem. Consider the following algorithm: First sort the liquid by their value per litre ($V_i/L_i$ for $i = 1, \dots, N$). Then, we successively pick the liquid with maximum value per litre and pour it into the bottle, until the bottle is filled or there is no unbottled liquid.

\item You should only stop at a gas station if, by the time you reach it, your remaining gas are not enough for you to reach the next gas station.

Now we prove that our strategy will yield an optimal set of gas stops that minimize the number of refills.

\begin{proof}
    We extend the notation so that the 0th ``gas station'' is USC and the $(n + 
    1)$th is Santa Monica. Let $i_1, \dots, i_m$ with $i_1 = 0$ and $i_m = n + 1$ be a set of stops that minimize the number of refills and $j_1, j_2, \dots$ with $j_1 = 0$ be the set of stops we make when we apply our strategy.

    First, we wish to prove that for $k = 1, \dots, m$, $j_k \ge i_k$ with mathematical induction.
    \begin{enumerate}
        \item The base case when $k = 1$ is apparent. We have $j_1 = 0 \ge i_1 = 0$.
        \item Given $j_k \ge i_k$, we need to prove $j_{k + 1} \ge i_{k + 1}$. There are two cases here with regard to the relative placement of the gas stations $j_k$ and $i_{k + 1}$.
        \begin{itemize}
            \item If $j_k \ge i_{k + 1}$, then we easily have $j_{k + 1} > j_k \ge i_{k + 1}$.
            \item If $j_k < i_{k + 1}$, the station $j_k$ is behind $i_{k + 1}$. Since $d_{i_{k + 1}} - d_{i_k} \le p$ and the gas station $j_k$ is not behind station $i_k$, we can reach $i_{k + 1}$ from $j_k$ while using the same amount as or even less gas than from $i_k$. By the time we reach $i_{k + 1}$ from $j_k$, either we still have enough gas to make it to the next station, giving $j_{k + 1} > i_{k + 1}$, or we simply can't make it, giving $j_{k + 1} = i_{k + 1}$. Either way, we have $j_{k + 1} \ge i_{k + 1}$.
        \end{itemize}
    \end{enumerate}
    
    Now that $j_k \ge i_k$ for $k = 1, \dots, m$, we have $j_m \ge i_m = n + 1$. As $j_k \le n + 1$ for all $k$, it must be the case that $j_m = i_m = n + 1$, showing that our strategy yield the same number of stops as an optimal set of gas stops.
\end{proof}

The time complexity of our algorithm is $\Theta(n)$, as the time to consider whether to stop for one gas station is constant and we need to consider for every gas station.

\item
\begin{enumerate}
    \item The algorithm is as follows: To make change for $n$ cents, we make change in descending order of denomination. We will only use coins with a smaller denomination only if the change can be made in coins with larger denomination.
    
    Now we prove the optimality of our algorithm.
    \begin{proof}
        Let $(c_1, c_2, c_3, c_4)$ be the change our algorithm returned for $n$ cents i.e., $c_1$ quarters, $c_2$ dimes, $c_3$ nickels and $c_4$ pennies, and $(c_1^*, c_2^*, c_3^*, c_4^*)$ be any set of coins that minimize the number of coins for $n$. We examine each denomination individually to show that they use the same number of coins.
        \begin{enumerate}
            \item Because we try to use maximum number of large denomination coins in our algorithm, we have $c_1^* \le c_1$. To prove their equality, we assume $c_1^* < c_1$ and show that we can always replace existing coins in the optimal solution with quarters to reduce the number of coins. As $c_1 > c_1^* \ge 0$, we have $c_1 \ge 1$ and $n \ge 25$.
            \begin{enumerate}
                \item When $c_1^* \ge 3$, we have more than 3 dimes in the optimal solution that can be replaced with a quarter and a nickel, reducing the total number of coins by one in the process and contradicting the optimality of the solution.
                \item When $c_1^* < 3$, to make change for $n \ge 5$ without using more dimes, one has to add at least one nickel or five pennies, and we can always handpick another 20 cents from the rest of the coins and replace these 25 cents with a quarter, contradicting the optimality of the solution.
            \end{enumerate}
            Due to these contradictions, we have $c_1 = c_1^*$ for quarters.
            \item Given $c_1 = c_1^*$ and our rule of using maximum number of coins of large denomination, we have $c_2 \ge c_2^*$ for dimes. We also assume, for the sake of contradiction, that $c_2 > c_2^*$. Because we use the same number of quarters in both solutions, we only need to consider $10 \le n < 25$. We apply the same argument above, that you can always find coins in the optimal solution worth 10 cents and replace it with a dime, contradicting the optimality of the solution.
            \item With $c_1 = c_1^*$ and $c_2 = c_2^*$, we can follow the same outline of our arguments above and show that $c_3 = c_3^*$. Because we are making change for the same $n$, it must be the case that $c_4 = c_4^*$, completing our proof.
        \end{enumerate}
    \end{proof}
    \item Consider using the set $\{15, 10, 1\}$ to make change for 20 cents. Our algorithm gives 6 coins with 1 15-cent and 6 pennies, but the optimal solution is two dimes.
\end{enumerate}

\item We modify the algorithm for computing a topological ordering of a DAG so that when it is run on an arbitrary graph and can't find a node without incoming edges to delete, it runs a cycle detection algorithm to output a cycle.

The cycle detection algorithm works as follows: Starting at any vertex, we keep following its incoming edge to a new vertex until we encountered a visited vertex. Because every vertex has incoming edges and the total number of vertices is finite, we will always end at a visited vertex.

Since both BFS and recovering of the cycle have a time complexity of $O(m + n)$, the time complexity of the whole algorithm is $O(m + n)$.

\item We present the following algorithm: For each event $e$ in the $S'$, we find the first event $e' = e$ in $S$, discard the part of $S$ from the beginning to $e'$ (including $e'$), and find the next matching event in $S$, until we reach the end of $S'$.

\item We schedule the tasks' first parts on Computer A in descending order of their second parts' execution time, and schedule a task's second part on Computer B as soon as its first is completed.

Now we prove that our algorithm yield an optimal schedule.
\begin{proof}
    In a schedule, define an inversion to be a pair of tasks $(i, j)$ where task $i$ is scheduled before $j$ on Computer A while $b_i < b_j$. Our algorithm returns a schedule without inversions, and we will prove that such a schedule is optimal, in the following propositions:
    \begin{enumerate}
        \item There is an optimal schedule without idle time.
        
        This is apparent, so the proof is omitted.
    
        \item All schedules without inversions have the same execution time.
        
        Two schedules, both without inversions, may differ in how they arrange two tasks with the same execution time in their second parts but different in first parts. Suppose that the two schedules differ in how they arrange tasks with their second part's execution time being $b$. Since these tasks' first parts are scheduled consecutively and the second parts are started as soon as their corresponding first parts are finished, the last task to finish is always the task's second part whose first part is ranked last on A, among the tasks whose second parts' execution time being $b$. That second part's finish time is always the total execution time of those tasks' first part plus $b$ and the total execution time of any tasks' first parts before them. The finish time is independent of the ordering of the tasks' first parts.

        \item If a schedule has any inversion, it must have a pair of neighboring tasks that form an inversion.
        
        The proof is similar to (4.9) (a).

        \item Swapping neighboring tasks in an inversion doesn't increase the total execution time of the schedule.
        
        To see why this is the case, consider an inversion $(i, j)$ where $i$ is scheduled immediately before $j$. Note that swapping neighboring tasks on Computer A doesn't change the total execution time of task $i$ and $j$'s first parts, so we are only concerned with the swap's effect on the finish time of tasks on Computer B. Let the first parts' finish time be $f$.

        After the swap, task $j$'s second part is scheduled earlier, so we only need to consider task $i$'s second part. Let $f_i$ be task $i$'s second part's finish time before the swap and $f_i'$ after. We use $s_i$ and $s_i'$ similarly but for start time. Because we start a task's second part immediately after its first part finishes, we have $s_j = s_i' = f$. It follows that
        $$
            s_j = f_j - b_j = s_i' = f_i' - b_i.
        $$

        Rearranging the terms in the equality, we have
        $$
            f_i' = f_j - b_j + b_i < f_j,
        $$
        the last inequality due to the property of inversion, $b_i < b_j$. Thus, after the swap, the task $i$'s second part will actually finish before the task $j$'s second part before the swap, leading to no increase in the total execution time.
    \end{enumerate}

    Given these statements, consider any optimal schedule with no idle time. We can eliminate all inversions in the schedule using a bubble sort-like procedure, swapping neighboring tasks until there are no inversions. The resulting schedule is optimal and has no inversions. Since all schedules without inversions have the same total execution time, all schedules without inversions are optimal.
\end{proof}

The algorithm can be implemented with a max-heap that compare tasks based on their second parts' execution time, so the algorithm's time complexity is $O(N\log N)$.

\end{enumerate}

\end{document}
